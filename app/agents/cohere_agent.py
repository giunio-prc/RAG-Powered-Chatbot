import logging
import os
from collections.abc import AsyncIterator

from cohere.errors import TooManyRequestsError as CohereTooManyRequestError
from dotenv import load_dotenv
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)
from langchain.schema import StrOutputParser
from langchain_cohere import ChatCohere
from langchain_core.runnables import RunnableSequence

from app.interfaces.agent import AIAgentInterface
from app.interfaces.errors import TooManyRequestsError

load_dotenv()


logger = logging.getLogger(__name__)


class CohereAgent(AIAgentInterface):
    chain: RunnableSequence

    def __init__(self):
        """
        Initializes the CohereAgent with a chat model, prompt templates, and a runnable response chain.
        """
        cohere_model = os.getenv("COHERE_MODEL")
        if not cohere_model:
            raise ValueError("COHERE_MODEL environment variable is not set.")

        model = ChatCohere(model=cohere_model)
        system_message_prompt = SystemMessagePromptTemplate.from_template(
            self.prompt_template
        )
        human_message_prompt = HumanMessagePromptTemplate.from_template(
            template="{question}"
        )
        chat_prompt_template = ChatPromptTemplate.from_messages(
            [system_message_prompt, human_message_prompt]
        )
        self.chain = chat_prompt_template | model | StrOutputParser()

    async def query_with_context(self, question: str, context: str) -> str:
        """
        Asynchronously queries the language model with a question and additional context, returning the generated response.

        Parameters:
            question (str): The input question to be answered.
            context (str): Supplementary information to provide context for the question.

        Returns:
            str: The response generated by the language model.
        """
        logger.debug("Question: %s", question)
        logger.debug("Context: %s", context)
        response = await self.chain.ainvoke({"question": question, "context": context})
        return response

    async def get_stream_response(
        self, question: str, context: str
    ) -> AsyncIterator[str]:
        try:
            async for message in self.chain.astream(
                {"question": question, "context": context}
            ):
                yield message
        except CohereTooManyRequestError as err:
            raise TooManyRequestsError(content=err.body)
